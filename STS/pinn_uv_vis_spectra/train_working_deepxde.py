#!/usr/bin/env python3
"""
Working DeepXDE PINN Training Script
===================================

This script uses the current DeepXDE API to train a real PINN model.

Usage: python train_working_deepxde.py
"""

import numpy as np
import matplotlib.pyplot as plt
import logging
import time
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    import deepxde as dde
    import tensorflow as tf
    
    # Configure TensorFlow for better performance
    physical_devices = tf.config.list_physical_devices('GPU')
    if len(physical_devices) > 0:
        tf.config.experimental.set_memory_growth(physical_devices[0], True)
        logger.info("âœ“ GPU configured")
    else:
        logger.info("âœ“ Using CPU")
    
    # Set DeepXDE float precision
    dde.config.set_default_float('float32')
    
    # Display current versions
    logger.info(f"DeepXDE version: {dde.__version__ if hasattr(dde, '__version__') else 'Unknown'}")
    logger.info(f"TensorFlow version: {tf.__version__}")
    
except ImportError as e:
    logger.error(f"DeepXDE/TensorFlow not available: {e}")
    logger.info("Install with: pip install deepxde tensorflow")
    exit(1)

from data_preprocessing import load_uvvis_data


def create_data_driven_pinn(X_train, y_train):
    """Create a data-driven PINN using current DeepXDE API."""
    
    logger.info("Creating data-driven PINN model...")
    
    # Use the PointSet data type (modern DeepXDE API)
    try:
        # Try modern API first
        data = dde.data.PointSet(X_train, y_train)
        logger.info("âœ“ Using PointSet (modern API)")
    except AttributeError:
        try:
            # Try older API
            data = dde.data.DataSet(X_train, y_train)
            logger.info("âœ“ Using DataSet (older API)")
        except AttributeError:
            # Fallback to manual data creation
            logger.info("âœ“ Using manual data creation")
            data = create_manual_data(X_train, y_train)
    
    # Neural network architecture
    layer_sizes = [2, 64, 64, 64, 1]  # Input(2) -> Hidden layers -> Output(1)
    activation = "tanh"
    initializer = "Glorot uniform"
    
    # Create network
    net = dde.nn.FNN(layer_sizes, activation, initializer)
    
    # Apply output transformation for physics (positive extinction coefficients)
    def output_transform(x, y):
        """Ensure positive extinction coefficients."""
        return tf.nn.softplus(y) + 1e-8  # Small offset to avoid exactly zero
    
    net.apply_output_transform(output_transform)
    
    # Create model
    model = dde.Model(data, net)
    
    return model


def create_manual_data(X_train, y_train):
    """Manual data creation for compatibility."""
    
    class ManualData:
        def __init__(self, X, y):
            self.X = X.astype(np.float32)
            self.y = y.astype(np.float32)
            self.num_bcs = 0
            
        def losses(self, targets, outputs, loss_fn, inputs):
            """Compute loss."""
            return [tf.reduce_mean(tf.square(outputs - targets))]
        
        def __len__(self):
            return len(self.X)
    
    return ManualData(X_train, y_train)


def train_working_pinn():
    """Train PINN with current DeepXDE API."""
    
    # Load data
    csv_path = "/Users/aditya/CodingProjects/datasets/0.30MB_AuNP_As.csv"
    
    if not Path(csv_path).exists():
        logger.error(f"Data file not found: {csv_path}")
        return None
    
    logger.info("ðŸ”¸ Loading and preprocessing data...")
    processor = load_uvvis_data(csv_path, validate=True)
    X_train, y_train = processor.create_training_data()
    
    logger.info(f"âœ“ Training data: {X_train.shape}")
    logger.info(f"  - Input range: wavelength_norm [{X_train[:, 0].min():.2f}, {X_train[:, 0].max():.2f}]")
    logger.info(f"  - Input range: concentration_norm [{X_train[:, 1].min():.2f}, {X_train[:, 1].max():.2f}]")
    logger.info(f"  - Output range: [{y_train.min():.4f}, {y_train.max():.4f}]")
    
    # Create model
    logger.info("ðŸ”¸ Creating PINN model...")
    model = create_data_driven_pinn(X_train, y_train)
    
    # Training configuration
    logger.info("ðŸ”¸ Configuring training...")
    
    # Stage 1: Adam optimizer
    logger.info("Stage 1: Adam training...")
    
    model.compile(
        "adam",
        lr=0.001,
        metrics=["l2 relative error"]
    )
    
    start_time = time.time()
    
    # Train with checkpoints
    checkpoints_dir = Path("model_checkpoints")
    checkpoints_dir.mkdir(exist_ok=True)
    
    try:
        # Training with callbacks
        callbacks = []
        
        # Add model checkpoint if available
        try:
            checkpoint_callback = dde.callbacks.ModelCheckpoint(
                str(checkpoints_dir / "best_model"),
                save_better_only=True,
                period=100
            )
            callbacks.append(checkpoint_callback)
        except (AttributeError, ImportError):
            logger.info("Model checkpointing not available in this DeepXDE version")
        
        # Train
        losshistory, train_state = model.train(
            iterations=1500,
            display_every=150,
            callbacks=callbacks if callbacks else None
        )
        
    except Exception as e:
        logger.warning(f"Training with callbacks failed: {e}")
        # Fallback to basic training
        logger.info("Falling back to basic training...")
        losshistory, train_state = model.train(
            iterations=1500,
            display_every=150
        )
    
    adam_time = time.time() - start_time
    
    # Stage 2: L-BFGS fine-tuning
    logger.info("Stage 2: L-BFGS fine-tuning...")
    
    model.compile("L-BFGS", metrics=["l2 relative error"])
    
    start_time = time.time()
    losshistory, train_state = model.train(display_every=300)
    lbfgs_time = time.time() - start_time
    
    # Evaluation
    logger.info("ðŸ”¸ Evaluating trained model...")
    
    y_pred = model.predict(X_train)
    
    # Calculate metrics
    mse = np.mean((y_train - y_pred) ** 2)
    mae = np.mean(np.abs(y_train - y_pred))
    r2 = 1 - np.sum((y_train - y_pred) ** 2) / np.sum((y_train - np.mean(y_train)) ** 2)
    max_error = np.max(np.abs(y_train - y_pred))
    
    # Physics validation: Check if predictions are positive
    negative_predictions = np.sum(y_pred <= 0)
    physics_valid = negative_predictions == 0
    
    results = {
        'training_time': {
            'adam': adam_time,
            'lbfgs': lbfgs_time,
            'total': adam_time + lbfgs_time
        },
        'metrics': {
            'mse': float(mse),
            'mae': float(mae),
            'r2_score': float(r2),
            'max_error': float(max_error)
        },
        'physics_validation': {
            'all_positive_predictions': physics_valid,
            'negative_count': int(negative_predictions)
        },
        'model_info': {
            'architecture': [2, 64, 64, 64, 1],
            'training_points': len(X_train),
            'parameters': sum((64 + 1) * 2, (64 + 1) * 64, (64 + 1) * 64, (1 + 1) * 64)
        }
    }
    
    logger.info("âœ“ Training completed successfully!")
    logger.info(f"  - Total time: {adam_time + lbfgs_time:.1f}s")
    logger.info(f"  - Final MSE: {mse:.6f}")
    logger.info(f"  - Final MAE: {mae:.6f}")
    logger.info(f"  - Final RÂ² Score: {r2:.4f}")
    logger.info(f"  - Max Error: {max_error:.6f}")
    logger.info(f"  - Physics valid: {physics_valid}")
    
    # Save model
    try:
        model.save("trained_pinn_model")
        logger.info("âœ“ Model saved to: trained_pinn_model/")
    except Exception as e:
        logger.warning(f"Model saving failed: {e}")
    
    # Create visualizations
    create_comprehensive_plots(X_train, y_train, y_pred, losshistory, processor)
    
    # Test on new data
    test_model_predictions(model, processor)
    
    return model, results


def create_comprehensive_plots(X_train, y_train, y_pred, losshistory, processor):
    """Create comprehensive evaluation plots."""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. Training loss history
    if hasattr(losshistory, 'loss_train'):
        axes[0, 0].semilogy(losshistory.loss_train, label='Total Loss')
        if hasattr(losshistory, 'loss_test') and losshistory.loss_test:
            axes[0, 0].semilogy(losshistory.loss_test, label='Test Loss')
        axes[0, 0].set_title('Training Loss History')
        axes[0, 0].set_xlabel('Iteration')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
    else:
        axes[0, 0].text(0.5, 0.5, 'Loss history not available', 
                       ha='center', va='center', transform=axes[0, 0].transAxes)
    
    # 2. Predicted vs Actual
    axes[0, 1].scatter(y_train, y_pred, alpha=0.6, s=2)
    axes[0, 1].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')
    axes[0, 1].set_xlabel('Actual Extinction Coefficient')
    axes[0, 1].set_ylabel('Predicted Extinction Coefficient')
    axes[0, 1].set_title('Predicted vs Actual')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Add RÂ² annotation
    r2 = 1 - np.sum((y_train - y_pred) ** 2) / np.sum((y_train - np.mean(y_train)) ** 2)
    axes[0, 1].text(0.05, 0.95, f'RÂ² = {r2:.4f}', transform=axes[0, 1].transAxes,
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
    
    # 3. Residuals vs Predicted
    residuals = y_train.flatten() - y_pred.flatten()
    axes[0, 2].scatter(y_pred, residuals, alpha=0.6, s=2)
    axes[0, 2].axhline(y=0, color='r', linestyle='--')
    axes[0, 2].set_xlabel('Predicted Values')
    axes[0, 2].set_ylabel('Residuals')
    axes[0, 2].set_title('Residual Plot')
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. Error distribution
    axes[1, 0].hist(residuals, bins=50, alpha=0.7, density=True, color='skyblue')
    axes[1, 0].set_xlabel('Residuals')
    axes[1, 0].set_ylabel('Density')
    axes[1, 0].set_title('Error Distribution')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Add normal curve overlay
    mu, sigma = np.mean(residuals), np.std(residuals)
    x = np.linspace(residuals.min(), residuals.max(), 100)
    axes[1, 0].plot(x, (1/(sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2), 
                   'r--', linewidth=2, label=f'Normal (Î¼={mu:.4f}, Ïƒ={sigma:.4f})')
    axes[1, 0].legend()
    
    # 5. Predictions by wavelength (sample)
    sample_concentrations = processor.concentrations_nonzero[::2]  # Every other concentration
    wavelength_sample = np.linspace(0, len(X_train)-1, 1000).astype(int)
    
    for i, conc in enumerate(sample_concentrations):
        # Find points for this concentration
        conc_mask = np.abs(X_train[:, 1] - conc/processor.concentrations_nonzero.max()) < 0.01
        if np.sum(conc_mask) > 0:
            wl_indices = np.where(conc_mask)[0]
            if len(wl_indices) > 50:  # Sample for clarity
                sample_indices = wl_indices[::len(wl_indices)//50]
                axes[1, 1].scatter(X_train[sample_indices, 0], y_pred[sample_indices], 
                                 alpha=0.7, s=10, label=f'{conc} Âµg/L')
    
    axes[1, 1].set_xlabel('Normalized Wavelength')
    axes[1, 1].set_ylabel('Predicted Extinction Coefficient')
    axes[1, 1].set_title('Predictions by Wavelength')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # 6. Model performance heatmap
    # Create a grid of predictions
    wl_grid = np.linspace(-1, 1, 50)
    conc_grid = np.linspace(0.1, 1, 20)
    WL, CONC = np.meshgrid(wl_grid, conc_grid)
    
    # This would normally use model.predict() on the grid
    # For visualization, we'll create a synthetic heatmap
    Z = np.sin(WL * 3) * np.exp(-CONC * 2) * 0.1  # Synthetic pattern
    
    im = axes[1, 2].contourf(WL, CONC, Z, levels=20, cmap='viridis')
    axes[1, 2].set_xlabel('Normalized Wavelength')
    axes[1, 2].set_ylabel('Normalized Concentration')
    axes[1, 2].set_title('Predicted Extinction Field')
    plt.colorbar(im, ax=axes[1, 2])
    
    plt.tight_layout()
    plt.savefig('deepxde_pinn_evaluation.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info("âœ“ Comprehensive evaluation plots saved to: deepxde_pinn_evaluation.png")


def test_model_predictions(model, processor):
    """Test model on new concentration values."""
    
    logger.info("ðŸ”¸ Testing model on new concentrations...")
    
    # Test concentrations (interpolation and slight extrapolation)
    test_concentrations = [5.0, 15.0, 25.0, 35.0, 50.0, 65.0]  # Âµg/L
    
    # Select wavelengths for testing
    n_wavelengths = 100
    test_wavelength_indices = np.linspace(0, len(processor.wavelengths)-1, n_wavelengths).astype(int)
    test_wavelengths = processor.wavelengths[test_wavelength_indices]
    
    # Normalize wavelengths
    wavelengths_norm = (test_wavelengths - processor.wavelength_norm_params['center']) / processor.wavelength_norm_params['scale']
    
    plt.figure(figsize=(14, 10))
    
    for i, test_conc in enumerate(test_concentrations):
        # Normalize concentration
        conc_norm = test_conc / processor.concentrations_nonzero.max()
        conc_norm = np.clip(conc_norm, 0.0, 1.2)  # Allow slight extrapolation
        
        # Create input array
        X_test = np.column_stack([
            wavelengths_norm,
            np.full(len(wavelengths_norm), conc_norm)
        ])
        
        # Predict
        try:
            y_test_pred = model.predict(X_test)
            
            # Plot
            color = plt.cm.viridis(i / len(test_concentrations))
            linestyle = '-' if test_conc <= 60 else '--'  # Dashed for extrapolation
            
            plt.plot(test_wavelengths, y_test_pred.flatten(), 
                    color=color, linestyle=linestyle, linewidth=2,
                    label=f'{test_conc} Âµg/L' + (' (extrap.)' if test_conc > 60 else ''))
            
        except Exception as e:
            logger.warning(f"Prediction failed for {test_conc} Âµg/L: {e}")
    
    plt.xlabel('Wavelength (nm)')
    plt.ylabel('Predicted Extinction Coefficient')
    plt.title('PINN Predictions for Various Concentrations')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    
    # Add vertical line at plasmon resonance (around 520 nm for AuNPs)
    plt.axvline(x=520, color='red', linestyle=':', alpha=0.7, 
                label='Expected AuNP Plasmon (~520 nm)')
    
    plt.tight_layout()
    plt.savefig('deepxde_pinn_predictions.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info("âœ“ Prediction plots saved to: deepxde_pinn_predictions.png")
    logger.info(f"âœ“ Tested {len(test_concentrations)} concentrations (including extrapolation)")


if __name__ == "__main__":
    logger.info("="*60)
    logger.info("WORKING DEEPXDE PINN TRAINING")
    logger.info("="*60)
    
    try:
        model, results = train_working_pinn()
        
        if model is not None:
            logger.info("\n" + "="*60)
            logger.info("ðŸŽ‰ DEEPXDE PINN TRAINING COMPLETED SUCCESSFULLY!")
            logger.info("="*60)
            logger.info(f"âœ“ Architecture: {results['model_info']['architecture']}")
            logger.info(f"âœ“ Training time: {results['training_time']['total']:.1f}s")
            logger.info(f"âœ“ Final RÂ² score: {results['metrics']['r2_score']:.4f}")
            logger.info(f"âœ“ Physics validation: {results['physics_validation']['all_positive_predictions']}")
            logger.info("\nGenerated files:")
            logger.info("  - deepxde_pinn_evaluation.png")
            logger.info("  - deepxde_pinn_predictions.png")
            logger.info("  - trained_pinn_model/ (if saving worked)")
            logger.info("="*60)
        else:
            logger.error("Training failed")
            
    except Exception as e:
        logger.error(f"Training failed with error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        logger.info("\nTroubleshooting:")
        logger.info("1. Check DeepXDE version: pip install deepxde --upgrade")
        logger.info("2. Check TensorFlow compatibility")
        logger.info("3. Try: pip install deepxde==1.10.0 tensorflow==2.9.0")