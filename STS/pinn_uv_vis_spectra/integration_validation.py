"""
Integration Validation for UV-Vis PINN Implementation
=====================================================

This script validates the integration between all phases of the UV-Vis PINN
implementation, testing compatibility and basic functionality without requiring
full DeepXDE/TensorFlow setup.

Validation checks:
- Data preprocessing and model definition compatibility
- Loss function interface consistency
- Training pipeline component integration
- API consistency across phases

"""

import numpy as np
import sys
import logging
from pathlib import Path
from typing import Dict, Any, List
import json
import traceback

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import our modules
try:
    from data_preprocessing import UVVisDataProcessor, load_uvvis_data
    from model_definition import SpectroscopyPINN, create_spectroscopy_pinn
except ImportError as e:
    logger.error(f"Failed to import modules: {e}")
    logger.error("Make sure all modules are in the Python path")
    sys.exit(1)


class IntegrationValidator:
    """
    Comprehensive integration validator for UV-Vis PINN implementation.
    
    Tests compatibility and integration between all phases without requiring
    external dependencies like DeepXDE or TensorFlow.
    """
    
    def __init__(self, csv_path: str):
        """
        Initialize validator with data path.
        
        Args:
            csv_path: Path to the UV-Vis data CSV file
        """
        self.csv_path = csv_path
        self.validation_results = {
            'phase_1_data_preprocessing': {},
            'phase_2_model_definition': {},
            'phase_3_loss_functions': {},
            'phase_4_training': {},
            'integration_tests': {},
            'overall_status': 'pending'
        }
        
        logger.info(f"Initialized integration validator for: {csv_path}")
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """
        Run comprehensive validation of all phases and their integration.
        
        Returns:
            Dictionary containing validation results for all phases
        """
        logger.info("Starting comprehensive integration validation...")
        
        try:
            # Phase 1: Data preprocessing validation
            self.validate_phase_1_data_preprocessing()
            
            # Phase 2: Model definition validation
            self.validate_phase_2_model_definition()
            
            # Phase 3: Loss function validation (interface only)
            self.validate_phase_3_loss_functions()
            
            # Phase 4: Training pipeline validation (interface only)
            self.validate_phase_4_training()
            
            # Integration tests
            self.validate_integration_compatibility()
            
            # Overall assessment
            self.assess_overall_status()
            
        except Exception as e:
            logger.error(f"Validation failed with error: {e}")
            logger.error(traceback.format_exc())
            self.validation_results['overall_status'] = 'failed'
            self.validation_results['error'] = str(e)
        
        return self.validation_results
    
    def validate_phase_1_data_preprocessing(self) -> None:
        """Validate Phase 1: Data preprocessing functionality."""
        logger.info("Validating Phase 1: Data Preprocessing...")
        
        phase_results = {
            'status': 'pending',
            'tests': {}
        }
        
        try:
            # Test 1: Basic data loading
            processor = UVVisDataProcessor(self.csv_path)
            processor.load_and_validate_data()
            phase_results['tests']['data_loading'] = 'passed'
            
            # Test 2: Spectral component extraction
            wavelengths, concentrations, absorbance_matrix = processor.extract_spectral_components()
            
            # Validate dimensions
            assert len(wavelengths) > 0, "No wavelengths extracted"
            assert len(concentrations) == 6, f"Expected 6 concentrations, got {len(concentrations)}"
            assert absorbance_matrix.shape == (len(wavelengths), len(concentrations)), \
                f"Absorbance matrix shape mismatch: {absorbance_matrix.shape}"
            
            phase_results['tests']['spectral_extraction'] = 'passed'
            phase_results['data_dimensions'] = {
                'wavelengths': len(wavelengths),
                'concentrations': len(concentrations),
                'wavelength_range': (float(wavelengths.min()), float(wavelengths.max())),
                'concentration_range': (float(concentrations.min()), float(concentrations.max()))
            }
            
            # Test 3: Beer-Lambert component computation
            baseline, differential = processor.compute_beer_lambert_components()
            
            assert baseline.shape[0] == len(wavelengths), "Baseline shape mismatch"
            assert differential.shape[0] == len(wavelengths), "Differential shape mismatch"
            assert differential.shape[1] == 5, f"Expected 5 differential concentrations, got {differential.shape[1]}"
            
            phase_results['tests']['beer_lambert_computation'] = 'passed'
            
            # Test 4: Input normalization
            normalized_data = processor.normalize_inputs()
            
            required_keys = ['wavelengths_norm', 'concentrations_norm', 'differential_absorption_norm']
            for key in required_keys:
                assert key in normalized_data, f"Missing normalized data key: {key}"
            
            # Check normalization bounds
            wl_norm = normalized_data['wavelengths_norm']
            conc_norm = normalized_data['concentrations_norm']
            
            assert wl_norm.min() >= -1.1 and wl_norm.max() <= 1.1, f"Wavelength normalization out of bounds: [{wl_norm.min()}, {wl_norm.max()}]"
            assert conc_norm.min() >= -0.1 and conc_norm.max() <= 1.1, f"Concentration normalization out of bounds: [{conc_norm.min()}, {conc_norm.max()}]"
            
            phase_results['tests']['normalization'] = 'passed'
            
            # Test 5: Training data creation
            X_train, y_train = processor.create_training_data()
            
            assert X_train.shape[1] == 2, f"Expected 2 input features, got {X_train.shape[1]}"
            assert y_train.shape[1] == 1, f"Expected 1 output feature, got {y_train.shape[1]}"
            assert len(X_train) == len(y_train), "Input-output length mismatch"
            
            phase_results['tests']['training_data_creation'] = 'passed'
            phase_results['training_data_shape'] = {
                'n_samples': len(X_train),
                'n_input_features': X_train.shape[1],
                'n_output_features': y_train.shape[1]
            }
            
            # Store processor for later tests
            self.data_processor = processor
            self.training_data = (X_train, y_train)
            
            phase_results['status'] = 'passed'
            
        except Exception as e:
            logger.error(f"Phase 1 validation failed: {e}")
            phase_results['status'] = 'failed'
            phase_results['error'] = str(e)
        
        self.validation_results['phase_1_data_preprocessing'] = phase_results
    
    def validate_phase_2_model_definition(self) -> None:
        """Validate Phase 2: Model definition functionality."""
        logger.info("Validating Phase 2: Model Definition...")
        
        phase_results = {
            'status': 'pending',
            'tests': {}
        }
        
        try:
            # Test 1: Basic PINN initialization
            pinn = SpectroscopyPINN(
                wavelength_range=(200, 800),
                concentration_range=(0, 60)
            )
            
            assert pinn.wavelength_range == (200, 800)
            assert pinn.concentration_range == (0, 60)
            assert pinn.path_length == 1.0
            
            phase_results['tests']['pinn_initialization'] = 'passed'
            
            # Test 2: Geometry creation
            geometry = pinn.create_geometry()
            
            # Basic geometry validation (without DeepXDE)
            assert geometry is not None
            assert hasattr(pinn, 'geometry')
            
            phase_results['tests']['geometry_creation'] = 'passed'
            
            # Test 3: Neural network architecture definition
            try:
                net = pinn.create_neural_network()
                phase_results['tests']['network_creation'] = 'passed'
            except ImportError:
                # DeepXDE not available - validate architecture parameters instead
                assert len(pinn.layer_sizes) >= 3, "Insufficient network depth"
                assert pinn.layer_sizes[0] == 2, "Input layer size should be 2"
                assert pinn.layer_sizes[-1] == 1, "Output layer size should be 1"
                phase_results['tests']['network_architecture'] = 'passed'
            
            # Test 4: Model summary generation
            summary = pinn.get_model_summary()
            
            required_sections = ['architecture', 'domain', 'physics', 'geometry']
            for section in required_sections:
                assert section in summary, f"Missing summary section: {section}"
            
            phase_results['tests']['model_summary'] = 'passed'
            phase_results['model_summary'] = summary
            
            # Test 5: Factory function
            pinn_factory = create_spectroscopy_pinn(architecture="standard")
            
            assert isinstance(pinn_factory, SpectroscopyPINN)
            assert pinn_factory.layer_sizes == [2, 64, 128, 128, 64, 32, 1]
            
            phase_results['tests']['factory_function'] = 'passed'
            
            # Store for later tests
            self.pinn_model = pinn
            
            phase_results['status'] = 'passed'
            
        except Exception as e:
            logger.error(f"Phase 2 validation failed: {e}")
            phase_results['status'] = 'failed'
            phase_results['error'] = str(e)
        
        self.validation_results['phase_2_model_definition'] = phase_results
    
    def validate_phase_3_loss_functions(self) -> None:
        """Validate Phase 3: Loss function interfaces."""
        logger.info("Validating Phase 3: Loss Function Interfaces...")
        
        phase_results = {
            'status': 'pending',
            'tests': {}
        }
        
        try:
            # Test interface compatibility without TensorFlow
            
            # Test 1: Loss function module import
            try:
                from loss_functions import UVVisLossFunction, create_uvvis_loss_function, AdaptiveLossWeighting
                phase_results['tests']['module_import'] = 'passed'
            except ImportError as e:
                logger.warning(f"Loss function module import failed (expected without TF): {e}")
                phase_results['tests']['module_import'] = 'skipped'
                phase_results['tests']['skip_reason'] = 'TensorFlow not available'
                phase_results['status'] = 'skipped'
                self.validation_results['phase_3_loss_functions'] = phase_results
                return
            
            # Test 2: Loss function initialization
            loss_fn = UVVisLossFunction(
                path_length=1.0,
                physics_weight=0.1,
                smooth_weight=1e-4
            )
            
            assert loss_fn.path_length == 1.0
            assert loss_fn.physics_weight == 0.1
            assert loss_fn.smooth_weight == 1e-4
            
            phase_results['tests']['loss_function_init'] = 'passed'
            
            # Test 3: Factory function
            factory_loss = create_uvvis_loss_function(
                path_length=2.0,
                physics_weight=0.2
            )
            
            assert callable(factory_loss)
            phase_results['tests']['factory_function'] = 'passed'
            
            # Test 4: Adaptive weighting
            adaptive_weighting = AdaptiveLossWeighting()
            
            assert hasattr(adaptive_weighting, 'current_weights')
            assert hasattr(adaptive_weighting, 'update_weights')
            
            phase_results['tests']['adaptive_weighting'] = 'passed'
            
            phase_results['status'] = 'passed'
            
        except Exception as e:
            logger.error(f"Phase 3 validation failed: {e}")
            phase_results['status'] = 'failed'
            phase_results['error'] = str(e)
        
        self.validation_results['phase_3_loss_functions'] = phase_results
    
    def validate_phase_4_training(self) -> None:
        """Validate Phase 4: Training pipeline interfaces."""
        logger.info("Validating Phase 4: Training Pipeline Interfaces...")
        
        phase_results = {
            'status': 'pending',
            'tests': {}
        }
        
        try:
            # Test interface compatibility without DeepXDE training
            
            # Test 1: Training module import
            try:
                from training import (
                    UVVisTrainingStrategy,
                    UVVisLeaveOneScanOutCV,
                    TrainingMetricsTracker,
                    create_training_pipeline
                )
                phase_results['tests']['module_import'] = 'passed'
            except ImportError as e:
                logger.warning(f"Training module import failed (expected without DeepXDE): {e}")
                phase_results['tests']['module_import'] = 'skipped'
                phase_results['status'] = 'skipped'
                self.validation_results['phase_4_training'] = phase_results
                return
            
            # Test 2: Training strategy initialization (without actual model)
            # This would normally require a DeepXDE model, so we skip detailed testing
            phase_results['tests']['training_strategy_interface'] = 'interface_validated'
            
            # Test 3: Cross-validation strategy initialization
            # Mock minimal requirements for CV strategy
            cv_strategy = UVVisLeaveOneScanOutCV(
                data_processor=self.data_processor,
                model_factory=lambda x: MockModel(),
                loss_factory=lambda: MockLoss(),
                cv_results_dir="test_cv"
            )
            
            assert cv_strategy.data_processor == self.data_processor
            assert len(cv_strategy.concentration_levels) > 0
            
            phase_results['tests']['cv_strategy_init'] = 'passed'
            
            # Test 4: Metrics tracker
            tracker = TrainingMetricsTracker()
            
            assert hasattr(tracker, 'metrics_history')
            assert hasattr(tracker, 'update_metrics')
            assert hasattr(tracker, 'get_summary')
            
            # Test metrics update
            tracker.update_metrics('test_stage', 100, {'loss': 0.5})
            summary = tracker.get_summary()
            
            assert 'test_stage' in tracker.metrics_history
            assert 'metrics_history' in summary
            
            phase_results['tests']['metrics_tracker'] = 'passed'
            
            # Test 5: Training pipeline interface
            # This requires mocked components but tests the interface
            phase_results['tests']['pipeline_interface'] = 'interface_validated'
            
            phase_results['status'] = 'passed'
            
        except Exception as e:
            logger.error(f"Phase 4 validation failed: {e}")
            phase_results['status'] = 'failed'
            phase_results['error'] = str(e)
        
        self.validation_results['phase_4_training'] = phase_results
    
    def validate_integration_compatibility(self) -> None:
        """Validate integration and compatibility between phases."""
        logger.info("Validating Integration Compatibility...")
        
        integration_results = {
            'status': 'pending',
            'tests': {}
        }
        
        try:
            # Test 1: Data format compatibility between Phase 1 and Phase 2
            if hasattr(self, 'data_processor') and hasattr(self, 'pinn_model'):
                # Check data dimensions match model expectations
                X_train, y_train = self.training_data
                
                # Model expects 2 input features (wavelength, concentration)
                assert X_train.shape[1] == 2, f"Model expects 2 inputs, data has {X_train.shape[1]}"
                
                # Model expects 1 output (epsilon prediction)
                assert y_train.shape[1] == 1, f"Model expects 1 output, data has {y_train.shape[1]}"
                
                integration_results['tests']['data_model_compatibility'] = 'passed'
            
            # Test 2: Normalization parameter consistency
            if hasattr(self, 'data_processor'):
                norm_data = self.data_processor.normalize_inputs()
                
                # Check that wavelength normalization is symmetric around 0
                wl_norm = norm_data['wavelengths_norm']
                wl_center = (wl_norm.min() + wl_norm.max()) / 2
                assert abs(wl_center) < 0.1, f"Wavelength normalization not centered: {wl_center}"
                
                # Check that concentration normalization starts from 0
                conc_norm = norm_data['concentrations_norm']
                assert conc_norm.min() >= -0.01, f"Concentration normalization below 0: {conc_norm.min()}"
                
                integration_results['tests']['normalization_consistency'] = 'passed'
            
            # Test 3: Physics parameter consistency
            if hasattr(self, 'pinn_model'):
                # Path length should be consistent
                expected_path_length = 1.0  # cm
                assert self.pinn_model.path_length == expected_path_length
                
                # Domain ranges should be reasonable for UV-Vis
                wl_range = self.pinn_model.wavelength_range
                assert wl_range[0] >= 100 and wl_range[1] <= 1000, f"Unrealistic wavelength range: {wl_range}"
                
                conc_range = self.pinn_model.concentration_range
                assert conc_range[0] >= 0 and conc_range[1] <= 1000, f"Unrealistic concentration range: {conc_range}"
                
                integration_results['tests']['physics_consistency'] = 'passed'
            
            # Test 4: API consistency across phases
            # Check that all phases follow similar initialization patterns
            apis_consistent = True
            
            # All main classes should have clear initialization parameters
            # All should provide summary/status methods
            if hasattr(self, 'data_processor'):
                assert hasattr(self.data_processor, 'get_data_summary')
            
            if hasattr(self, 'pinn_model'):
                assert hasattr(self.pinn_model, 'get_model_summary')
            
            integration_results['tests']['api_consistency'] = 'passed'
            
            # Test 5: Error handling compatibility
            # Check that errors are handled consistently across phases
            try:
                # Test invalid data processor initialization
                invalid_processor = UVVisDataProcessor("nonexistent_file.csv")
                try:
                    invalid_processor.load_and_validate_data()
                    assert False, "Should have raised FileNotFoundError"
                except FileNotFoundError:
                    pass  # Expected
                
                integration_results['tests']['error_handling'] = 'passed'
                
            except Exception as e:
                logger.warning(f"Error handling test failed: {e}")
                integration_results['tests']['error_handling'] = 'warning'
            
            integration_results['status'] = 'passed'
            
        except Exception as e:
            logger.error(f"Integration validation failed: {e}")
            integration_results['status'] = 'failed'
            integration_results['error'] = str(e)
        
        self.validation_results['integration_tests'] = integration_results
    
    def assess_overall_status(self) -> None:
        """Assess overall validation status based on individual phase results."""
        logger.info("Assessing Overall Validation Status...")
        
        phase_statuses = []
        critical_failures = []
        
        for phase_name, phase_result in self.validation_results.items():
            if phase_name == 'overall_status':
                continue
            
            status = phase_result.get('status', 'unknown')
            phase_statuses.append(status)
            
            if status == 'failed':
                critical_failures.append(phase_name)
        
        # Determine overall status
        if 'failed' in phase_statuses:
            overall_status = 'failed'
            logger.error(f"Validation FAILED. Critical failures in: {critical_failures}")
        elif 'pending' in phase_statuses:
            overall_status = 'incomplete'
            logger.warning("Validation INCOMPLETE. Some phases still pending.")
        elif all(status in ['passed', 'skipped'] for status in phase_statuses):
            overall_status = 'passed'
            logger.info("Validation PASSED. All phases completed successfully.")
        else:
            overall_status = 'unknown'
            logger.warning("Validation status UNKNOWN. Mixed results.")
        
        self.validation_results['overall_status'] = overall_status
        self.validation_results['summary'] = {
            'total_phases': len(phase_statuses),
            'passed': phase_statuses.count('passed'),
            'failed': phase_statuses.count('failed'),
            'skipped': phase_statuses.count('skipped'),
            'critical_failures': critical_failures
        }
    
    def save_validation_report(self, output_path: str = "validation_report.json") -> None:
        """Save comprehensive validation report."""
        logger.info(f"Saving validation report to: {output_path}")
        
        try:
            # Make results JSON serializable
            serializable_results = self._make_json_serializable(self.validation_results)
            
            with open(output_path, 'w') as f:
                json.dump(serializable_results, f, indent=2)
            
            logger.info(f"Validation report saved successfully to {output_path}")
            
        except Exception as e:
            logger.error(f"Failed to save validation report: {e}")
    
    def _make_json_serializable(self, obj):
        """Convert numpy arrays and other non-serializable objects to JSON-compatible types."""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        else:
            return obj
    
    def print_validation_summary(self) -> None:
        """Print a human-readable validation summary."""
        print("\n" + "="*60)
        print("UV-VIS PINN INTEGRATION VALIDATION SUMMARY")
        print("="*60)
        
        overall_status = self.validation_results.get('overall_status', 'unknown')
        summary = self.validation_results.get('summary', {})
        
        print(f"Overall Status: {overall_status.upper()}")
        print(f"Total Phases: {summary.get('total_phases', 0)}")
        print(f"Passed: {summary.get('passed', 0)}")
        print(f"Failed: {summary.get('failed', 0)}")
        print(f"Skipped: {summary.get('skipped', 0)}")
        
        if summary.get('critical_failures'):
            print(f"Critical Failures: {', '.join(summary['critical_failures'])}")
        
        print("\nPhase-by-Phase Results:")
        print("-" * 30)
        
        phase_names = {
            'phase_1_data_preprocessing': 'Phase 1: Data Preprocessing',
            'phase_2_model_definition': 'Phase 2: Model Definition',
            'phase_3_loss_functions': 'Phase 3: Loss Functions',
            'phase_4_training': 'Phase 4: Training Pipeline',
            'integration_tests': 'Integration Tests'
        }
        
        for phase_key, phase_name in phase_names.items():
            phase_result = self.validation_results.get(phase_key, {})
            status = phase_result.get('status', 'unknown')
            
            status_icon = {
                'passed': '✓',
                'failed': '✗',
                'skipped': '⚠',
                'pending': '⏳',
                'unknown': '?'
            }.get(status, '?')
            
            print(f"{status_icon} {phase_name}: {status.upper()}")
            
            # Show test details
            tests = phase_result.get('tests', {})
            if tests:
                for test_name, test_result in tests.items():
                    test_icon = '✓' if test_result == 'passed' else ('⚠' if test_result == 'skipped' else '✗')
                    print(f"  {test_icon} {test_name}: {test_result}")
        
        print("\n" + "="*60)


# Mock classes for testing interfaces without external dependencies
class MockModel:
    """Mock model for interface testing."""
    def predict(self, X):
        return np.random.rand(len(X), 1)


class MockLoss:
    """Mock loss function for interface testing."""
    def __call__(self, *args):
        return 0.1


def main():
    """Main validation execution."""
    # Default CSV path
    csv_path = "/Users/aditya/CodingProjects/datasets/0.30MB_AuNP_As.csv"
    
    # Check if CSV exists
    if not Path(csv_path).exists():
        logger.error(f"CSV file not found: {csv_path}")
        logger.info("Please ensure the UV-Vis data file is available at the expected location")
        return
    
    # Create validator
    validator = IntegrationValidator(csv_path)
    
    # Run validation
    results = validator.run_comprehensive_validation()
    
    # Print summary
    validator.print_validation_summary()
    
    # Save detailed report
    validator.save_validation_report("integration_validation_report.json")
    
    # Return success/failure for CI/CD
    overall_status = results.get('overall_status', 'unknown')
    if overall_status == 'passed':
        logger.info("Integration validation PASSED")
        return 0
    elif overall_status == 'failed':
        logger.error("Integration validation FAILED")
        return 1
    else:
        logger.warning("Integration validation INCOMPLETE")
        return 2


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)